{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Datachecker","text":""},{"location":"#quickstart","title":"Quickstart","text":"<p><pre><code>pip install git+https://github.com/ONSdigital/datachecker.git\n</code></pre> create a schema for your dataset (either in python or read in the supported formats). Load in your dataset. Create a new validator object using </p> <p><pre><code>from datachecker import check_and_export\nvalidator = check_and_export(\n    df=dataframe,\n    schema=\"path/to_schema.json\",\n    output=\"html\",\n    output_name=\"my_log\"\n)\n</code></pre> This will then directly validate your dataset and produce a log. If your schema is not the correct format or missing some key values, Python errors will be given.</p>"},{"location":"#pre-defined-checks","title":"Pre-Defined Checks","text":"<p>These checks can be included in the lists for individual columns in your schema, depending on the data type.</p> Data Type Check Name Parameter Check Definition integer / double Minimum value min_val Checks that all values are above or equal to the minimum value integer / double Maximum value max_val Checks that all values are below or equal to the maximum value character Minimum length min_length Checks that all strings have length are above or equal to the minimum length character Maximum length max_length Checks that all strings have length below or equal to the maximum length character allowed strings allowed_strings Validates that entries match a set of permitted values, list or regex can be used. (Optional and can use forbidden strings instead) any Missing values check allow_na Checks for missing or NA values in the column. double Minimum decimal places min_decimal Checks that all values have more or equal amounts of decimal places double Maximum decimal places max_decimal Checks that all values have less or equal amounts of decimal places character forbidden strings forbidden_strings Validates that entries do not contain a set of forbidden values, list can be used. (Optional and can use allowed strings instead. Does not support regex to use regex we recommend using allowed_characters. A TypeError message will be provided with further details) date / datetime Minimum Date min_date Checks that all dates are after the minimum date using the format \u201cYYYY-MM-DD\u201d date / datetime Maximum Date max_date Checks that all dates are before the maximum date using the format \u201cYYYY-MM-DD\u201d date/ datetime Minimum Datetime min_datetime Checks that all dates are after the minimum datetime. Accepted formats: Y, YM, YMD, YMDH, YMDHM and YMDHMS date/ datetime Maximum Datetime max_datetime Checks that all dates are before the maximum datetime. Accepted formats: Y, YM, YMD, YMDH, YMDHM and YMDHMS"},{"location":"#custom-checks","title":"Custom Checks","text":"<p>The ability to add custom checks is supported through pandera using lambda functions.  Custom checks cannot be defined in the main schema and must instead be defined as its own dictionary in your python script. Then when creating your <code>DataValidator</code> object, simply pass this as an additional argument and your custom check will be applied across the entire dataframe. </p> <p>Note</p> <p>You will get a log entry per column for this check, even for columns that are not contained in your custom check.</p> <pre><code>my_custom_checks = {\n    \"my_custom_check_name\" : lambda df: (df[\"column_1\"] &lt; 100) &amp; (df[\"column_2\"].isna())\n}\n\nnew_validator = DataValidator(\n    schema = schema, \n    data=df,\n    file = \"output_report.yaml\",\n    format=\"yaml\",\n    custom_checks = my_custom_checks)\n\nnew_validator.validate()\n</code></pre>"},{"location":"#checks-that-are-not-supported-currently","title":"Checks that are not supported currently.","text":"Data Type Check Name Parameter Check Definition any Class class Checks that column data Class matches the specified type"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog,  and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#changed","title":"Changed","text":""},{"location":"changelog/#removed","title":"Removed","text":""},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#100-yyyy-mm-dd","title":"[1.0.0] - YYYY-MM-DD","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release.</li> </ul>"},{"location":"api_reference/checks/","title":"Checks","text":"<p>Warning</p> <p>This section is a work in progress</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.allowed_strings","title":"<code>allowed_strings(value)</code>","text":"<p>Create a pandera check for allowed strings.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.allowed_strings--parameters","title":"Parameters","text":"<p>value : list | str     The list of allowed strings or regex pattern.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.allowed_strings--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the allowed strings.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def allowed_strings(value: list | str):\n    \"\"\"\n    Create a pandera check for allowed strings.\n\n    Parameters\n    ----------\n    value : list | str\n        The list of allowed strings or regex pattern.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the allowed strings.\n    \"\"\"\n    if isinstance(value, str):\n        return pa.Check.str_matches(value)\n    elif isinstance(value, list):\n        return pa.Check.isin(value)\n    else:\n        raise TypeError(\"allowed_strings value must be a list or string\")\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema","title":"<code>convert_schema(schema, custom_checks=None)</code>","text":"<p>Convert the loaded schema to a pandera DataFrameSchema. Uses simple defined functions to map schema keys to pandera checks. To add further checks, define schema key function above and add to the loop within this function.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema--parameters","title":"Parameters","text":"<p>schema : dict     The schema to convert.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema--returns","title":"Returns","text":"<p>pa.DataFrameSchema     The converted pandera DataFrameSchema.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def convert_schema(schema: dict, custom_checks: dict = None) -&gt; pa.DataFrameSchema:\n    \"\"\"\n    Convert the loaded schema to a pandera DataFrameSchema. Uses simple defined\n    functions to map schema keys to pandera checks. To add further checks, define\n    schema key function above and add to the loop within this function.\n\n    Parameters\n    ----------\n    schema : dict\n        The schema to convert.\n\n    Returns\n    -------\n    pa.DataFrameSchema\n        The converted pandera DataFrameSchema.\n    \"\"\"\n    # Convert JSON schema to pandera schema\n    # Loop over each column in the JSON schema and create corresponding pandera Column objects\n    pa_schema_format = {}\n    for column_name, constraints in schema[\"columns\"].items():\n        column_type = constraints[\"type\"]\n        nullable = constraints.get(\"allow_na\", False)\n        checks = []\n        if \"min_val\" in constraints:\n            checks.append(min_val(constraints[\"min_val\"]))\n        if \"max_val\" in constraints:\n            checks.append(max_val(constraints[\"max_val\"]))\n        if \"min_length\" in constraints and column_type is str:\n            checks.append(string_length(min_length=constraints[\"min_length\"]))\n        if \"max_length\" in constraints and column_type is str:\n            checks.append(string_length(max_length=constraints[\"max_length\"]))\n        if \"allowed_strings\" in constraints and column_type is str:\n            checks.append(allowed_strings(constraints[\"allowed_strings\"]))\n        if \"forbidden_strings\" in constraints and column_type is str:\n            checks.append(forbidden_strings(constraints[\"forbidden_strings\"]))\n        if \"min_decimal\" in constraints and column_type is float:\n            checks.append(min_decimal(constraints[\"min_decimal\"]))\n        if \"max_decimal\" in constraints and column_type is float:\n            checks.append(max_decimal(constraints[\"max_decimal\"]))\n        if (\n            \"max_date\" in constraints or \"max_datetime\" in constraints\n        ) and column_type is pd.Timestamp:\n            checks.append(max_date(constraints.get(\"max_date\", constraints.get(\"max_datetime\"))))\n        if (\n            \"min_date\" in constraints or \"min_datetime\" in constraints\n        ) and column_type is pd.Timestamp:\n            checks.append(min_date(constraints.get(\"min_date\", constraints.get(\"min_datetime\"))))\n\n        pa_type = column_type\n\n        pa_schema_format[column_name] = pa.Column(dtype=pa_type, checks=checks, nullable=nullable)\n\n    if custom_checks is not None:\n        formatted_custom_checks = format_custom_checks(custom_checks)\n    else:\n        formatted_custom_checks = []\n\n    return pa.DataFrameSchema(pa_schema_format, checks=formatted_custom_checks)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema_into_log_entries","title":"<code>convert_schema_into_log_entries(converted_schema)</code>","text":"<p>converts pandera schema into log entries dataframe for all checks defined in schema Used to create a complete log of all checks, including those that pass.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema_into_log_entries--parameters","title":"Parameters","text":"<p>converted_schema : pa.DataFrameSchema     The pandera DataFrameSchema to convert.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.convert_schema_into_log_entries--returns","title":"Returns","text":"<p>pd.DataFrame | None     A dataframe of log entries or None if no checks are defined.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def convert_schema_into_log_entries(converted_schema: pa.DataFrameSchema) -&gt; pd.DataFrame | None:\n    \"\"\"\n    converts pandera schema into log entries dataframe for all checks defined in schema\n    Used to create a complete log of all checks, including those that pass.\n\n    Parameters\n    ----------\n    converted_schema : pa.DataFrameSchema\n        The pandera DataFrameSchema to convert.\n\n    Returns\n    -------\n    pd.DataFrame | None\n        A dataframe of log entries or None if no checks are defined.\n    \"\"\"\n    # allow_na (nullable) checks are not included in .checks\n    # need to find a way to include these in the log output\n    dict_column_checks = {}\n    for i in converted_schema.columns:\n        dict_column_checks[i] = converted_schema.columns[i].checks + converted_schema.checks\n\n    if not dict_column_checks:\n        # message = f\"Checking {i} {temp[0].error}\"\n        return None\n    else:\n        list_of_checks = []\n        list_of_columns = []\n        for col, checks in dict_column_checks.items():\n            for check in checks:\n                list_of_checks.append(check.error)\n                list_of_columns.append(col)\n        for col in converted_schema.columns:\n            data_type = converted_schema.columns[col].dtype\n            list_of_checks.append(f\"dtype('{data_type}')\")\n            list_of_columns.append(col)\n        return pd.DataFrame(\n            {\n                \"check\": list_of_checks,\n                \"column\": list_of_columns,\n                \"invalid_ids\": [[]] * len(list_of_checks),\n            }\n        )\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.forbidden_strings","title":"<code>forbidden_strings(value)</code>","text":"<p>Create a pandera check for forbidden strings.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.forbidden_strings--parameters","title":"Parameters","text":"<p>value : list | str     The list of forbidden strings.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.forbidden_strings--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the forbidden strings.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.forbidden_strings--raises","title":"Raises","text":"<p>TypeError     If the value is a string. Regex is not supported for forbidden_strings or general     TypeError if value is not a list or string.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def forbidden_strings(value: list):\n    \"\"\"\n    Create a pandera check for forbidden strings.\n\n    Parameters\n    ----------\n    value : list | str\n        The list of forbidden strings.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the forbidden strings.\n\n    Raises\n    ------\n    TypeError\n        If the value is a string. Regex is not supported for forbidden_strings or general\n        TypeError if value is not a list or string.\n    \"\"\"\n    if isinstance(value, list):\n        return pa.Check.notin(value)\n    if isinstance(value, str):\n        raise TypeError(\n            \"String patterns are not supported for forbidden_strings, \"\n            \"please use either a list or a regex pattern in allowed_strings.\"\n        )\n    else:\n        raise TypeError(\"forbidden_strings value must be a list or string\")\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_date","title":"<code>max_date(value)</code>","text":"<p>Create a pandera check for maximum date.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_date--parameters","title":"Parameters","text":"<p>value : str     The maximum date or datetime in 'YYYY-MM-DD HH:MM' or equivalent format.     YYYYMMDDHHMM is also accepted but recommend separating with - or / for clarity.     Date format with no timestamp is also accepted.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_date--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the maximum date.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def max_date(value: str):\n    \"\"\"\n    Create a pandera check for maximum date.\n\n    Parameters\n    ----------\n    value : str\n        The maximum date or datetime in 'YYYY-MM-DD HH:MM' or equivalent format.\n        YYYYMMDDHHMM is also accepted but recommend separating with - or / for clarity.\n        Date format with no timestamp is also accepted.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the maximum date.\n    \"\"\"\n    max_date_value = pd.to_datetime(value)\n    return pa.Check.le(max_date_value)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_decimal","title":"<code>max_decimal(value)</code>","text":"<p>Create a pandera check for maximum decimal places.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_decimal--parameters","title":"Parameters","text":"<p>value : int     The maximum number of decimal places.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_decimal--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the maximum decimal places.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def max_decimal(value: int):\n    \"\"\"\n    Create a pandera check for maximum decimal places.\n\n    Parameters\n    ----------\n    value : int\n        The maximum number of decimal places.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the maximum decimal places.\n    \"\"\"\n    return pa.Check(\n        lambda s: s.apply(\n            lambda x: (\n                isinstance(x, float) and not pd.isnull(x) and len(str(x).split(\".\")[1]) &lt;= value\n            )\n            if isinstance(x, float) and not pd.isnull(x)\n            else True\n        ),\n        element_wise=False,\n        error=f\"has at most {value} decimal places\",\n    )\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_val","title":"<code>max_val(value)</code>","text":"<p>Create a pandera check for maximum value.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_val--parameters","title":"Parameters","text":"<p>value : int | float     The maximum value to check against.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.max_val--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the maximum value.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def max_val(value: int | float):\n    \"\"\"\n    Create a pandera check for maximum value.\n\n    Parameters\n    ----------\n    value : int | float\n        The maximum value to check against.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the maximum value.\n    \"\"\"\n    return pa.Check.le(value)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_date","title":"<code>min_date(value)</code>","text":"<p>Create a pandera check for minimum date.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_date--parameters","title":"Parameters","text":"<p>value : str     The minimum date in 'YYYY-MM-DD HH:MM' or equivalent format.     YYYYMMDDHHMM is also accepted but recommend separating with - or / for clarity.     Date format with no timestamp is also accepted.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_date--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the minimum date.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def min_date(value: str):\n    \"\"\"\n    Create a pandera check for minimum date.\n\n    Parameters\n    ----------\n    value : str\n        The minimum date in 'YYYY-MM-DD HH:MM' or equivalent format.\n        YYYYMMDDHHMM is also accepted but recommend separating with - or / for clarity.\n        Date format with no timestamp is also accepted.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the minimum date.\n    \"\"\"\n    min_date_value = pd.to_datetime(value)\n    return pa.Check.ge(min_date_value)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_decimal","title":"<code>min_decimal(value)</code>","text":"<p>Create a pandera check for minimum decimal places for floats (possible with pandera decimal data type)</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_decimal--parameters","title":"Parameters","text":"<p>value : int     The minimum number of decimal places.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_decimal--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the minimum decimal places.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def min_decimal(value: int):\n    \"\"\"\n    Create a pandera check for minimum decimal places for floats (possible with pandera\n    decimal data type)\n\n    Parameters\n    ----------\n    value : int\n        The minimum number of decimal places.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the minimum decimal places.\n    \"\"\"\n    return pa.Check(\n        lambda s: s.apply(\n            lambda x: (\n                isinstance(x, float) and not pd.isnull(x) and len(str(x).split(\".\")[1]) &gt;= value\n            )\n            if isinstance(x, float) and not pd.isnull(x)\n            else True\n        ),\n        element_wise=False,\n        error=f\"has at least {value} decimal places\",\n    )\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_val","title":"<code>min_val(value)</code>","text":"<p>Create a pandera check for minimum value.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_val--parameters","title":"Parameters","text":"<p>value : int | float     The minimum value to check against.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.min_val--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the minimum value.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def min_val(value: int | float):\n    \"\"\"\n    Create a pandera check for minimum value.\n\n    Parameters\n    ----------\n    value : int | float\n        The minimum value to check against.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the minimum value.\n    \"\"\"\n    return pa.Check.ge(value)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.string_length","title":"<code>string_length(max_length=None, min_length=None)</code>","text":"<p>Create a pandera check for string length.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.string_length--parameters","title":"Parameters","text":"<p>max_length : int | None     The maximum length of the string. min_length : int | None     The minimum length of the string.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.string_length--returns","title":"Returns","text":"<p>pa.Check     A pandera check for the string length.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def string_length(max_length: int = None, min_length: int = None):\n    \"\"\"\n    Create a pandera check for string length.\n\n    Parameters\n    ----------\n    max_length : int | None\n        The maximum length of the string.\n    min_length : int | None\n        The minimum length of the string.\n\n    Returns\n    -------\n    pa.Check\n        A pandera check for the string length.\n    \"\"\"\n    return pa.Check.str_length(max_value=max_length, min_value=min_length)\n</code></pre>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.validate_using_pandera","title":"<code>validate_using_pandera(converted_schema, data)</code>","text":"<p>validate data using a pandera DataFrameSchema. Returns a dataframe of failed checks with columns: 'column', 'check', 'failure_case', 'invalid_ids'. If all checks pass, returns None.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.validate_using_pandera--parameters","title":"Parameters","text":"<p>converted_schema : pa.DataFrameSchema     The pandera DataFrameSchema to use for validation. data : pd.DataFrame     The data to validate.</p>"},{"location":"api_reference/checks/#datachecker.checks_loaders_and_exporters.checks.validate_using_pandera--returns","title":"Returns","text":"<p>pd.DataFrame | None     A dataframe of failed checks or None if all checks pass.</p> Source code in <code>datachecker/checks_loaders_and_exporters/checks.py</code> <pre><code>def validate_using_pandera(\n    converted_schema: pa.DataFrameSchema, data: pd.DataFrame\n) -&gt; pd.DataFrame | None:\n    \"\"\"\n    validate data using a pandera DataFrameSchema. Returns a dataframe of failed checks\n    with columns: 'column', 'check', 'failure_case', 'invalid_ids'.\n    If all checks pass, returns None.\n\n    Parameters\n    ----------\n    converted_schema : pa.DataFrameSchema\n        The pandera DataFrameSchema to use for validation.\n    data : pd.DataFrame\n        The data to validate.\n\n    Returns\n    -------\n    pd.DataFrame | None\n        A dataframe of failed checks or None if all checks pass.\n    \"\"\"\n    # ISSUE - NA pass not present in output log\n\n    try:\n        converted_schema.validate(data, lazy=True)\n\n        # The following code is to add all checks when validation passes\n        grouped_validation_return = None\n    except pa.errors.SchemaErrors as e:\n        # validation_return is now a pandas dataframe\n        validation_return = e.failure_cases[[\"column\", \"check\", \"failure_case\", \"index\"]]\n        # Group by 'column' and 'check', collect failure cases and indices for each group\n        grouped_validation_return = (\n            validation_return.groupby([\"column\", \"check\"])\n            .agg({\"failure_case\": list, \"index\": list})\n            .reset_index()\n            .rename(columns={\"index\": \"invalid_ids\"})\n        )\n    passing_tests = convert_schema_into_log_entries(converted_schema=converted_schema)\n    combined = pd.concat([grouped_validation_return, passing_tests], ignore_index=True)\n    # Drop duplicates\n    combined = combined.drop_duplicates([\"column\", \"check\"], keep=\"first\")\n    # Sort by order in passing_tests[\"column\"]\n    if passing_tests is not None and not passing_tests.empty:\n        col_order_passing = passing_tests[\"column\"].unique().tolist()\n        # make sure we grab failed column names if no passing test for that column\n        if grouped_validation_return is None:\n            col_order_failed = []\n        else:\n            col_order_failed = grouped_validation_return[\"column\"].unique().tolist()\n        col_order = col_order_passing + [\n            col for col in col_order_failed if col not in col_order_passing\n        ]\n        combined[\"column\"] = pd.Categorical(combined[\"column\"], categories=col_order, ordered=True)\n        combined = combined.sort_values(\"column\").reset_index(drop=True)\n    return combined\n</code></pre>"},{"location":"api_reference/main/","title":"Main","text":"<p>Warning</p> <p>This section is a work in progress</p>"},{"location":"api_reference/main/#datachecker.main.DataValidator","title":"<code>DataValidator</code>","text":"<p>               Bases: <code>Validator</code></p> <p>DataValidator is a subclass of Validator specifically for validating data. It inherits all methods and attributes from the Validator class.</p> Source code in <code>datachecker/main.py</code> <pre><code>class DataValidator(Validator):\n    \"\"\"\n    DataValidator is a subclass of Validator specifically for validating data.\n    It inherits all methods and attributes from the Validator class.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema: dict,\n        data: pd.DataFrame,\n        file: str,\n        format: str,\n        hard_check: bool = True,\n        custom_checks: dict = None,\n    ):\n        super().__init__(schema, data, file, format, hard_check, custom_checks)\n\n    def validate(self):\n        for check in (\n            self._check_colnames,\n            self._check_column_contents,\n        ):\n            check()\n        # Formatting to convert pandera descriptions to more readable format\n        self._format_log_descriptions()\n        self._convert_frame_wide_check_to_single_entry()\n        return self\n\n    def _validate_schema(self, schema):\n        schema = super()._validate_schema(schema)\n        # Additional checks specific to DataValidator\n        df_columns = set(self.data.columns)\n        schema_keys = set(schema[\"columns\"].keys())\n        # if not df_columns.issubset(schema_keys):\n        missing = df_columns - schema_keys\n        self._add_qa_entry(\n            description=\"Dataframe columns missing from schema\",\n            failing_ids=list(missing),\n            outcome=not missing,\n            entry_type=\"error\",\n        )\n        # if not schema_keys.issubset(df_columns):\n        extra = schema_keys - df_columns\n        self._add_qa_entry(\n            description=\"Schema keys not in dataframe\",\n            failing_ids=list(extra),\n            outcome=not extra,\n            entry_type=\"warning\",\n        )\n\n        # Only mandatory entry inside columns is \"allow_na\"\n        for col, item in schema[\"columns\"].items():\n            item_keys = list(item.keys())\n            required_keys = [\"type\", \"allow_na\", \"optional\"]\n            if not all(key in item_keys for key in required_keys):\n                # missing_key_values = True\n                self._add_qa_entry(\n                    description=(\n                        f\"Missing required properties in schema for column '{col}': \"\n                        f\"{[key for key in required_keys if key not in item_keys]}\"\n                    ),\n                    failing_ids=[col],\n                    outcome=False,\n                    entry_type=\"error\",\n                )\n\n        self._check_unused_schema_arguments(schema)\n\n        return schema\n\n    def _check_colnames(self):\n        # Check column names do not contain symbols other than underscore or spaces\n        invalid_cols = [\n            col for col in self.data.columns if not all(c.isalnum() or c in [\"_\"] for c in col)\n        ]\n        self._add_qa_entry(\n            description=\"Checking column names\",\n            failing_ids=invalid_cols,\n            outcome=not invalid_cols,\n            entry_type=\"error\",\n        )\n\n        # Check column names are all lowercase\n        uppercase_cols = [col for col in self.data.columns if any(c.isupper() for c in col)]\n        self._add_qa_entry(\n            description=\"Checking column names are lowercase\",\n            failing_ids=uppercase_cols,\n            outcome=not uppercase_cols,\n            entry_type=\"warning\",\n        )\n\n        # Check mandatory columns are present\n        missing_mandatory = [\n            col\n            for col, props in self.schema.get(\"columns\", {}).items()\n            if not props.get(\"optional\", False) and col not in self.data.columns\n        ]\n        self._add_qa_entry(\n            description=\"Checking mandatory columns are present\",\n            failing_ids=missing_mandatory,\n            outcome=not missing_mandatory,\n            entry_type=\"error\",\n        )\n\n        # Check no unexpected columns are present\n        unexpected_cols = [\n            col for col in self.data.columns if col not in self.schema.get(\"columns\", {})\n        ]\n        self._add_qa_entry(\n            description=\"Checking for unexpected columns\",\n            failing_ids=unexpected_cols,\n            outcome=not unexpected_cols,\n            entry_type=\"warning\",\n        )\n\n    def _check_column_contents(self, converted_schema=None):\n        # code to pass through converted schema. helps unit testing\n        if converted_schema is None:\n            converted_schema = convert_schema(self.schema, self.custom_checks)\n        grouped_validation_return = validate_using_pandera(converted_schema, data=self.data)\n        # Issue, only failed data type checks are returned from pandera validation\n        if grouped_validation_return is not None:\n            for i in grouped_validation_return.index:\n                entry_description = (\n                    f\"Checking {grouped_validation_return.at[i, 'column']} \"\n                    + f\"{grouped_validation_return.at[i, 'check']}\"\n                )\n                invalid_ids = grouped_validation_return.at[i, \"invalid_ids\"]\n                if invalid_ids == [None]:\n                    invalid_ids = grouped_validation_return.at[i, \"failure_case\"]\n\n                self._add_qa_entry(\n                    description=entry_description,\n                    failing_ids=invalid_ids,\n                    outcome=not bool(invalid_ids),\n                    entry_type=\"error\",\n                )\n\n    def _check_unused_schema_arguments(self, schema):\n        # Unused arguments in schema.\n        valid_schema_keys = {\n            \"type\",\n            \"min_val\",\n            \"max_val\",\n            \"min_length\",\n            \"max_length\",\n            \"allowed_strings\",\n            \"forbidden_strings\",\n            \"allow_na\",\n            \"optional\",\n            \"min_decimal\",\n            \"max_decimal\",\n            \"max_date\",\n            \"min_date\",\n            \"max_datetime\",\n            \"min_datetime\",\n        }\n        unpacked_keys = []\n        for _col, item in schema[\"columns\"].items():\n            unpacked_keys.extend(item.keys())\n        unpacked_keys = set(unpacked_keys)\n        unused_keys = unpacked_keys.difference(valid_schema_keys)\n        self._add_qa_entry(\n            description=\"Checking for unused arguments in schema\",\n            failing_ids=list(unused_keys),\n            outcome=not unused_keys,\n            entry_type=\"warning\",\n        )\n</code></pre>"},{"location":"api_reference/main/#datachecker.main.Validator","title":"<code>Validator</code>","text":"<p>Validator class for validating data against a specified schema.</p>"},{"location":"api_reference/main/#datachecker.main.Validator--attributes","title":"Attributes","text":"<p>log : list     Stores log entries for validation steps and outcomes. schema : dict or object     The loaded schema used for validation. data : any     The data to be validated. file : str     The file path for exporting validation logs. format : str     The format to use when exporting logs. hard_check : bool     Determines if strict validation is enforced.</p>"},{"location":"api_reference/main/#datachecker.main.Validator--methods","title":"Methods","text":"<p>init(schema, data, file, format, hard_check=True)     Initializes the Validator with schema, data, file, format, and validation strictness. validate()     Runs a series of validation checks on the data, including column names, types, and contents. add_qa_entry(description, outcome, entry_type=\"info\")     Adds a QA log entry with a description, outcome, and entry type (default is \"info\"). export()     Exports the validation log using the specified format and file path. repr()     Returns a string representation of the Validator instance. str()     Returns a formatted string with schema information and the validation log. validate_schema(schema)     Loads and validates the schema from a file path or returns it if already loaded.</p> Source code in <code>datachecker/main.py</code> <pre><code>class Validator:\n    \"\"\"\n    Validator class for validating data against a specified schema.\n\n    Attributes\n    ----------\n    log : list\n        Stores log entries for validation steps and outcomes.\n    schema : dict or object\n        The loaded schema used for validation.\n    data : any\n        The data to be validated.\n    file : str\n        The file path for exporting validation logs.\n    format : str\n        The format to use when exporting logs.\n    hard_check : bool\n        Determines if strict validation is enforced.\n\n    Methods\n    -------\n    __init__(schema, data, file, format, hard_check=True)\n        Initializes the Validator with schema, data, file, format, and validation strictness.\n    validate()\n        Runs a series of validation checks on the data, including column names, types, and contents.\n    add_qa_entry(description, outcome, entry_type=\"info\")\n        Adds a QA log entry with a description, outcome, and entry type (default is \"info\").\n    export()\n        Exports the validation log using the specified format and file path.\n    __repr__()\n        Returns a string representation of the Validator instance.\n    __str__()\n        Returns a formatted string with schema information and the validation log.\n    validate_schema(schema)\n        Loads and validates the schema from a file path or returns it if already loaded.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema: dict,\n        data: pd.DataFrame,\n        file: str,\n        format: str,\n        hard_check: bool = True,\n        custom_checks: dict = None,\n    ):\n        self.log = self._create_log()\n        self.data = data\n        self.file = file\n        self.format = format\n        self.hard_check = hard_check\n        self._validate_and_assign_custom_checks(custom_checks)\n        self.schema = self._validate_schema(schema)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        # Create a table header\n        sys_info = \"\\n\".join([f\"{key}: {value}\" for key, value in self.log[0].items()])\n        headers = [\n            \"Timestamp\",\n            \"Status\",\n            \"Description\",\n            \"Outcome\",\n            \"Failing IDs\",\n            \"Number Failing\",\n        ]\n        header_row = \" | \".join(headers)\n        separator = \"-|-\".join([\"-\" * len(h) for h in headers])\n        # Create table rows\n        rows = []\n        for entry in self.log[1:]:\n            row = [\n                entry.get(\"timestamp\", \"\"),\n                entry.get(\"status\", \"\").upper(),\n                entry.get(\"description\", \"\"),\n                entry.get(\"outcome\", \"\"),\n                \", \".join(map(str, entry.get(\"failing_ids\", []))),\n                str(entry.get(\"number_failing\", \"\")),\n            ]\n            rows.append(\" | \".join(row))\n        log_entries = \"\\n\".join([sys_info, \"\\n\", header_row, separator] + rows)\n        return log_entries if log_entries else \"No log entries.\"\n\n    def export(self):\n        Exporter.export(self.log, self.format, self.file)\n        self._hard_check_status()\n\n    def _validate_and_assign_custom_checks(self, custom_checks):\n        if custom_checks is not None:\n            if not isinstance(custom_checks, dict):\n                raise TypeError(\"custom_checks must be a dictionary of check_name: function pairs.\")\n            for name, func in custom_checks.items():\n                if not callable(func):\n                    raise TypeError(f\"Custom check '{name}' is not callable.\")\n        self.custom_checks = custom_checks\n\n    def _create_log(self):\n        sys_info = {\n            \"date\": pd.Timestamp.now().strftime(\"%Y-%m-%d\"),\n            \"user\": getpass.getuser(),\n            \"device\": platform.node(),\n            \"device_platform\": platform.platform(),\n            \"architecture\": platform.architecture()[0],\n            \"python_version\": platform.python_version(),\n            \"pandas_version\": pd.__version__,\n            \"pandera_version\": pandera.__version__,\n            \"datachecker_version\": version(\"datachecker\"),\n        }\n        return [sys_info]\n\n    def _add_qa_entry(self, description, failing_ids, outcome, entry_type=\"info\"):\n        outcome = \"pass\" if outcome else \"fail\"\n        if entry_type not in [\"info\", \"error\", \"warning\"]:\n            raise ValueError(\"entry_type must be 'info', 'error', or 'warning'.\")\n        if failing_ids is not None:\n            n_failing = len(failing_ids)\n            if len(failing_ids) &gt; 10:\n                failing_ids = failing_ids[:10] + [\"...\"]\n        else:\n            failing_ids = []\n            n_failing = None\n        timestamp = pd.Timestamp.now().strftime(\"%H:%M:%S\")\n\n        log_entry = {\n            \"timestamp\": timestamp,\n            \"description\": description,\n            \"outcome\": outcome,\n            \"failing_ids\": failing_ids,\n            \"number_failing\": n_failing,\n            \"status\": entry_type,\n        }\n\n        self.log.append(log_entry)\n\n    def _validate_schema(self, schema):\n        if not isinstance(schema, (str, dict)):\n            raise ValueError(\"Schema must be a file path (str) or a loaded schema (dict).\")\n\n        if isinstance(schema, str):\n            format = schema.split(\".\")[-1]\n            schema = SchemaLoader.load(schema, format)\n\n        return schema\n\n    def _hard_check_status(self):\n        error_count = 0\n        warning_count = 0\n        for entry in self.log[1:]:\n            if entry[\"status\"] == \"error\" and entry[\"outcome\"] == \"fail\":\n                error_count += 1\n            elif entry[\"status\"] == \"warning\" and entry[\"outcome\"] == \"fail\":\n                warning_count += 1\n\n        # Always raise a warning for the number of warnings, if any\n        if warning_count &gt; 0:\n            warnings.warn(\n                f\"Soft checks failed: {warning_count} warning(s) found, \"\n                \"see log output for more details\",\n                UserWarning,\n                stacklevel=2,\n            )\n        if self.hard_check and error_count &gt; 0:\n            raise ValueError(\n                f\"Hard checks failed: {error_count} error(s) found, see log output for more details\"\n            )\n        elif error_count &gt; 0:\n            warnings.warn(\n                f\"Soft checks failed: {error_count} warning(s) found, \"\n                \"see log output for more details\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def _format_log_descriptions(self):\n        # Optional method to format log descriptions for better readability\n\n        regex_replacements = [\n            (r\"str_length\\(\\s*(\\d+)\\s*,\\s*None\\s*\\)\", r\"string length greater than or equal to \\1\"),\n            (r\"str_length\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\", r\"string length between \\1 and \\2\"),\n            (r\"str_length\\(\\s*None\\s*,\\s*(\\d+)\\s*\\)\", r\"string length less than or equal to \\1\"),\n            (r\"dtype\\('(\\S+)'\\)\", r\"is data type \\1\"),\n            (r\"isin\\(\\s*\\[([^\\]]+)\\]\\s*\\)\", r\"contains only [\\1]\"),\n            (r\"str_matches\\(\\s*r?['\\\"](.*?)['\\\"]\\s*\\)\", r\"string matches pattern '\\1'\"),\n            (r\"greater_than_or_equal_to\\(\\s*(\\d+)\\s*\\)\", r\"greater than or equal to \\1\"),\n            (r\"less_than_or_equal_to\\(\\s*(\\d+)\\s*\\)\", r\"less than or equal to \\1\"),\n            (r\"less_than_or_equal_to\\(\\s*(\\S{10}\\s+\\S{8})\\s*\\)\", r\"before or equal to \\1\"),\n            (r\"greater_than_or_equal_to\\(\\s*(\\S{10}\\s+\\S{8})\\s*\\)\", r\"after or equal to \\1\"),\n            # Add more regex patterns as needed\n        ]\n        for entry in self.log[1:]:\n            # Bulk replace items in description using a dictionary\n            desc = entry[\"description\"]\n            for pattern, repl in regex_replacements:\n                desc = re.sub(pattern, repl, desc)\n\n            entry[\"description\"] = desc\n\n    def _convert_frame_wide_check_to_single_entry(self):\n        # Want to take any repeated log entries i.e. custom checks and convert to a single\n        # Log entry with description \"Custom data check {check_name}\"\n\n        # escape if no custom checks\n        if self.custom_checks is None:\n            return\n\n        log_entries = self.log[1:]  # Exclude system info\n        log_df = pd.DataFrame(log_entries)\n        grouped = pd.DataFrame()\n        for custom_check in self.custom_checks:\n            pattern = rf\"\\b{re.escape(custom_check)}\\b\"\n            custom_check_entries = log_df[\n                log_df[\"description\"].str.contains(pattern, case=False)\n            ].copy()\n            custom_check_entries[\"custom_check_name\"] = custom_check\n            # Drop the row from log_df before further processing\n            # Only match exact custom_check as a whole word in description\n            log_df = log_df[~log_df[\"description\"].str.contains(pattern, case=False)]\n            if not custom_check_entries.empty:\n                grouped = pd.concat([grouped, custom_check_entries])\n\n        if not grouped.empty:\n            single_entry = grouped.groupby([\"custom_check_name\"], as_index=False).first()\n            single_entry[\"description\"] = single_entry[\"custom_check_name\"].apply(\n                lambda x: f\"Custom data check {x}\"\n            )\n            # Convert grouped DataFrame back to list of dicts\n            wide_checks_entries = single_entry.drop(columns=[\"custom_check_name\"]).to_dict(\n                orient=\"records\"\n            )\n            # Remove old custom check entries from log\n            self.log = (\n                [self.log[0]]\n                + [\n                    entry\n                    for entry in self.log[1:]\n                    if not any(\n                        entry.get(\"description\", \"\").lower().find(custom_check.lower()) != -1\n                        for custom_check in self.custom_checks\n                    )\n                ]\n                + wide_checks_entries\n            )\n</code></pre>"},{"location":"api_reference/main/#datachecker.main.check_and_export","title":"<code>check_and_export(schema, data, file, format, hard_check=True, custom_checks=None)</code>","text":"<p>function to create validation object, run validation and export log.</p>"},{"location":"api_reference/main/#datachecker.main.check_and_export--parameters","title":"Parameters","text":"<p>schema : dict     The schema to validate against. data : pd.DataFrame | pl.DataFrame     The data to validate. file : str     The file path to export the validation log. format : str     The format to use when exporting the log. hard_check : bool, optional     Whether to perform strict validation checks, by default True custom_checks : list, optional     A list of custom validation checks to apply, by default None</p>"},{"location":"api_reference/main/#datachecker.main.check_and_export--returns","title":"Returns","text":"<p>DataValidator     Returns data validator object after validation and export.</p> Source code in <code>datachecker/main.py</code> <pre><code>def check_and_export(\n    schema, data, file, format, hard_check=True, custom_checks=None\n) -&gt; DataValidator:\n    \"\"\"\n    function to create validation object, run validation and export log.\n\n    Parameters\n    ----------\n    schema : dict\n        The schema to validate against.\n    data : pd.DataFrame | pl.DataFrame\n        The data to validate.\n    file : str\n        The file path to export the validation log.\n    format : str\n        The format to use when exporting the log.\n    hard_check : bool, optional\n        Whether to perform strict validation checks, by default True\n    custom_checks : list, optional\n        A list of custom validation checks to apply, by default None\n\n    Returns\n    -------\n    DataValidator\n        Returns data validator object after validation and export.\n    \"\"\"\n    if type(data) is pl.DataFrame:\n        warnings.warn(\n            \"Polars Dataframes are not natively supported, converting to Pandas for validation.\",\n            stacklevel=2,\n        )\n        data = data.to_pandas()\n    validator = DataValidator(\n        schema=schema,\n        data=data,\n        file=file,\n        format=format,\n        hard_check=hard_check,\n        custom_checks=custom_checks,\n    )\n\n    validator.validate()\n    validator.export()\n    return validator\n</code></pre>"},{"location":"api_reference/schema_loader/","title":"Schema Loader","text":"<p>Warning</p> <p>This section is a work in progress</p>"},{"location":"api_reference/schema_loader/#datachecker.checks_loaders_and_exporters.schema_loader.JSONSchemaLoader","title":"<code>JSONSchemaLoader</code>","text":"<p>               Bases: <code>SchemaLoader</code></p> <p>A schema loader for JSON files. This class extends the <code>SchemaLoader</code> to provide functionality for loading schemas defined in JSON format. It registers itself with the identifier \"json\" and uses the <code>json.load</code> method to parse JSON files.</p>"},{"location":"api_reference/schema_loader/#datachecker.checks_loaders_and_exporters.schema_loader.JSONSchemaLoader--methods","title":"Methods","text":"<p>init():     Initializes the loader and registers the JSON loading function. _load(schema: str) -&gt; dict:     Static method that loads and parses a JSON schema file.     Parameters     ----------     schema : str         The file path to the JSON schema.     Returns     -------     dict         The parsed schema as a Python dictionary.</p> Source code in <code>datachecker/checks_loaders_and_exporters/schema_loader.py</code> <pre><code>class JSONSchemaLoader(SchemaLoader):\n    \"\"\"\n    A schema loader for JSON files.\n    This class extends the `SchemaLoader` to provide functionality for loading\n    schemas defined in JSON format. It registers itself with the identifier \"json\"\n    and uses the `json.load` method to parse JSON files.\n\n    Methods\n    -------\n    __init__():\n        Initializes the loader and registers the JSON loading function.\n    _load(schema: str) -&gt; dict:\n        Static method that loads and parses a JSON schema file.\n        Parameters\n        ----------\n        schema : str\n            The file path to the JSON schema.\n        Returns\n        -------\n        dict\n            The parsed schema as a Python dictionary.\n    \"\"\"\n\n    def __init__(self):\n        SchemaLoader.__init__(self, \"json\", self._load)\n\n    @staticmethod\n    def _load(schema):\n        with open(schema, \"r\") as f:\n            return json.load(f)\n</code></pre>"},{"location":"api_reference/schema_loader/#datachecker.checks_loaders_and_exporters.schema_loader.SchemaLoader","title":"<code>SchemaLoader</code>","text":"<p>SchemaLoader is a class for registering and loading schema parsing functions based on format. Class Attributes:     format_dictionary (dict): A mapping from format names (str) to schema loader functions. Methods</p> <p>init(self, format, schema_loader_function):     Registers a schema loader function for a given format. load(cls, schema, format):     Loads and parses a schema using the registered loader function for the specified format.     Parameters     ----------     schema : str         The file path to the schema.     format : str         The format identifier (e.g., \"json\", \"yaml\").     Returns     -------     dict         The parsed schema as a Python dictionary.     Raises     ------     ValueError         If the requested format is not registered in format_dictionary.</p> Source code in <code>datachecker/checks_loaders_and_exporters/schema_loader.py</code> <pre><code>class SchemaLoader:\n    \"\"\"\n    SchemaLoader is a class for registering and loading schema parsing functions based on format.\n    Class Attributes:\n        format_dictionary (dict): A mapping from format names (str) to schema loader functions.\n    Methods\n    -------\n    __init__(self, format, schema_loader_function):\n        Registers a schema loader function for a given format.\n    load(cls, schema, format):\n        Loads and parses a schema using the registered loader function for the specified format.\n        Parameters\n        ----------\n        schema : str\n            The file path to the schema.\n        format : str\n            The format identifier (e.g., \"json\", \"yaml\").\n        Returns\n        -------\n        dict\n            The parsed schema as a Python dictionary.\n        Raises\n        ------\n        ValueError\n            If the requested format is not registered in format_dictionary.\n    \"\"\"\n\n    format_dictionary = {}\n\n    def __init__(self, format, schema_loader_function):\n        type(self).format_dictionary[format] = schema_loader_function\n\n    @classmethod\n    def load(cls, schema, format):\n        if format not in cls.format_dictionary:\n            raise ValueError(f\"Format '{format}' is not supported.\")\n        output_function = cls.format_dictionary[format]\n        return output_function(schema)\n</code></pre>"},{"location":"api_reference/schema_loader/#datachecker.checks_loaders_and_exporters.schema_loader.TOMLSchemaLoader","title":"<code>TOMLSchemaLoader</code>","text":"<p>               Bases: <code>SchemaLoader</code></p> <p>A schema loader for TOML files. This class extends the <code>SchemaLoader</code> to provide functionality for loading schemas defined in TOML format. It registers itself with the identifier \"toml\" and uses the <code>tomli.load</code> method to parse TOML files. Methods</p> <p>init():     Initializes the loader and registers the TOML loading function. _load(schema: str) -&gt; dict:     Static method that loads and parses a TOML schema file.     Parameters     ----------     schema : str         The file path to the TOML schema.     Returns     -------     dict         The parsed schema as a Python dictionary.</p> Source code in <code>datachecker/checks_loaders_and_exporters/schema_loader.py</code> <pre><code>class TOMLSchemaLoader(SchemaLoader):\n    \"\"\"\n    A schema loader for TOML files.\n    This class extends the `SchemaLoader` to provide functionality for loading\n    schemas defined in TOML format. It registers itself with the identifier \"toml\"\n    and uses the `tomli.load` method to parse TOML files.\n    Methods\n    -------\n    __init__():\n        Initializes the loader and registers the TOML loading function.\n    _load(schema: str) -&gt; dict:\n        Static method that loads and parses a TOML schema file.\n        Parameters\n        ----------\n        schema : str\n            The file path to the TOML schema.\n        Returns\n        -------\n        dict\n            The parsed schema as a Python dictionary.\n    \"\"\"\n\n    def __init__(self):\n        SchemaLoader.__init__(self, \"toml\", self._load)\n\n    @staticmethod\n    def _load(schema):\n        with open(schema, \"rb\") as f:\n            return tomli.load(f)\n</code></pre>"},{"location":"api_reference/schema_loader/#datachecker.checks_loaders_and_exporters.schema_loader.YAMLSchemaLoader","title":"<code>YAMLSchemaLoader</code>","text":"<p>               Bases: <code>SchemaLoader</code></p> <p>A schema loader for YAML files. This class extends the <code>SchemaLoader</code> to provide functionality for loading schemas defined in YAML format. It registers itself with the identifier \"yaml\" and uses the <code>yaml.safe_load</code> method to parse YAML files. Methods</p> <p>init():     Initializes the loader and registers the YAML loading function. _load(schema: str) -&gt; dict:     Static method that loads and parses a YAML schema file.     Parameters     ----------     schema : str         The file path to the YAML schema.     Returns     -------     dict         The parsed schema as a Python dictionary.</p> Source code in <code>datachecker/checks_loaders_and_exporters/schema_loader.py</code> <pre><code>class YAMLSchemaLoader(SchemaLoader):\n    \"\"\"\n    A schema loader for YAML files.\n    This class extends the `SchemaLoader` to provide functionality for loading\n    schemas defined in YAML format. It registers itself with the identifier \"yaml\"\n    and uses the `yaml.safe_load` method to parse YAML files.\n    Methods\n    -------\n    __init__():\n        Initializes the loader and registers the YAML loading function.\n    _load(schema: str) -&gt; dict:\n        Static method that loads and parses a YAML schema file.\n        Parameters\n        ----------\n        schema : str\n            The file path to the YAML schema.\n        Returns\n        -------\n        dict\n            The parsed schema as a Python dictionary.\n    \"\"\"\n\n    def __init__(self):\n        SchemaLoader.__init__(self, \"yaml\", self._load)\n\n    @staticmethod\n    def _load(schema):\n        with open(schema, \"r\") as f:\n            return yaml.safe_load(f)\n</code></pre>"},{"location":"api_reference/validator_exporter/","title":"Validator Exporter","text":"<p>Warning</p> <p>This section is a work in progress</p>"},{"location":"api_reference/validator_exporter/#datachecker.checks_loaders_and_exporters.validator_exporter.CSVExporter","title":"<code>CSVExporter</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>CSVExporter is a subclass of Exporter that provides functionality to export data to a CSV file. Methods</p> <p>init():     Initializes the CSVExporter by registering the \"csv\" export format and its handler. staticmethod _export(data, file):     Exports the provided data to a CSV file.     Parameters     ----------     data : list or dict         The data to be exported, typically a list of dictionaries or a dictionary.     file : str or file-like object         The file path or file-like object where the CSV will be written.     Returns     -------     str         A message indicating the file has been exported.</p> Source code in <code>datachecker/checks_loaders_and_exporters/validator_exporter.py</code> <pre><code>class CSVExporter(Exporter):\n    \"\"\"\n    CSVExporter is a subclass of Exporter that provides functionality to export data to a CSV file.\n    Methods\n    -------\n    __init__():\n        Initializes the CSVExporter by registering the \"csv\" export format and its handler.\n    _staticmethod_ _export(data, file):\n        Exports the provided data to a CSV file.\n        Parameters\n        ----------\n        data : list or dict\n            The data to be exported, typically a list of dictionaries or a dictionary.\n        file : str or file-like object\n            The file path or file-like object where the CSV will be written.\n        Returns\n        -------\n        str\n            A message indicating the file has been exported.\n    \"\"\"\n\n    def __init__(self):\n        Exporter.__init__(self, \"csv\", self._export)\n\n    @staticmethod\n    def _export(data, file):\n        data[0] = {\"timestamp\": \"\", \"description\": data[0]}\n        df = pd.DataFrame(data)\n        df.to_csv(file, index=False)\n        return f\"{file} exported\"\n</code></pre>"},{"location":"api_reference/validator_exporter/#datachecker.checks_loaders_and_exporters.validator_exporter.Exporter","title":"<code>Exporter</code>","text":"<p>Exporter class for registering and exporting data in various formats. This class allows you to register exporter functions for different formats and use them to export data. Exporter functions should accept two arguments: <code>data</code> (the data to export) and <code>file</code> (the file or file-like object to write to). Attributes:     format_dictionary (dict): A class-level dictionary mapping format names to     exporter functions. Methods:     init(self, format, exporter_function):         Registers a new exporter function for the specified format.     export(cls, data, format, file):         Exports the given data using the exporter function registered for the specified format.         Raises:             ValueError: If the specified format is not supported.</p> Source code in <code>datachecker/checks_loaders_and_exporters/validator_exporter.py</code> <pre><code>class Exporter:\n    \"\"\"\n    Exporter class for registering and exporting data in various formats.\n    This class allows you to register exporter functions for different formats and use them\n    to export data.\n    Exporter functions should accept two arguments: `data` (the data to export) and `file`\n    (the file or file-like object to write to).\n    Attributes:\n        format_dictionary (dict): A class-level dictionary mapping format names to\n        exporter functions.\n    Methods:\n        __init__(self, format, exporter_function):\n            Registers a new exporter function for the specified format.\n        export(cls, data, format, file):\n            Exports the given data using the exporter function registered for the specified format.\n            Raises:\n                ValueError: If the specified format is not supported.\n    \"\"\"\n\n    format_dictionary = {}\n\n    def __init__(self, format, exporter_function):\n        type(self).format_dictionary[format] = exporter_function\n\n    @classmethod\n    def export(cls, data, format, file):\n        if format not in cls.format_dictionary:\n            raise ValueError(f\"Format '{format}' is not supported.\")\n        output_function = cls.format_dictionary[format]\n        return output_function(data, file)\n</code></pre>"},{"location":"api_reference/validator_exporter/#datachecker.checks_loaders_and_exporters.validator_exporter.JSONExporter","title":"<code>JSONExporter</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>JSONExporter is a subclass of Exporter that handles exporting validation logs to a JSON file. Methods</p> <p>init():     Initializes the JSONExporter by registering the \"json\" export format and the associated     export method. _export(data, file):     Static method that writes the provided data to a file in JSON format.     Parameters:         data (Any): The validation log data to be exported.         file (str): The path to the file where the data will be written.     Returns:         str: A message indicating the file has been exported.</p> Source code in <code>datachecker/checks_loaders_and_exporters/validator_exporter.py</code> <pre><code>class JSONExporter(Exporter):\n    \"\"\"\n    JSONExporter is a subclass of Exporter that handles exporting validation logs to a JSON file.\n    Methods\n    -------\n    __init__():\n        Initializes the JSONExporter by registering the \"json\" export format and the associated\n        export method.\n    _export(data, file):\n        Static method that writes the provided data to a file in JSON format.\n        Parameters:\n            data (Any): The validation log data to be exported.\n            file (str): The path to the file where the data will be written.\n        Returns:\n            str: A message indicating the file has been exported.\n    \"\"\"\n\n    def __init__(self):\n        Exporter.__init__(self, \"json\", self._export)\n\n    @staticmethod\n    def _export(data, file):\n        data = {\"validation_log\": data}\n        with open(file, \"w\") as f:\n            json.dump(data, f, indent=4)\n        return f\"{file} exported\"\n</code></pre>"},{"location":"api_reference/validator_exporter/#datachecker.checks_loaders_and_exporters.validator_exporter.TXTExporter","title":"<code>TXTExporter</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>TXTExporter is a subclass of Exporter that handles exporting data to a plain text (.txt) file. Methods</p> <p>init():     Initializes the TXTExporter by registering the \"txt\" format and its export method. staticmethod _export(data, file):     Exports the provided data to a text file, writing each item on a new line. Parameters</p> <p>data : iterable     The data to be exported, where each item will be written as a separate line in the file. file : str or PathLike     The path to the file where the data will be exported. Returns</p> <p>str     A message indicating the file has been exported.</p> Source code in <code>datachecker/checks_loaders_and_exporters/validator_exporter.py</code> <pre><code>class TXTExporter(Exporter):\n    \"\"\"\n    TXTExporter is a subclass of Exporter that handles exporting data to a plain text\n    (.txt) file.\n    Methods\n    -------\n    __init__():\n        Initializes the TXTExporter by registering the \"txt\" format and its export method.\n    _staticmethod_ _export(data, file):\n        Exports the provided data to a text file, writing each item on a new line.\n    Parameters\n    ----------\n    data : iterable\n        The data to be exported, where each item will be written as a separate line in the file.\n    file : str or PathLike\n        The path to the file where the data will be exported.\n    Returns\n    -------\n    str\n        A message indicating the file has been exported.\n    \"\"\"\n\n    def __init__(self):\n        Exporter.__init__(self, \"txt\", self._export)\n\n    @staticmethod\n    def _export(data, file):\n        with open(file, \"w\") as f:\n            for item in data:\n                if isinstance(item, (dict, list)):\n                    formatted = json.dumps(item, indent=4)\n                    f.write(f\"{formatted}\\n\")\n                else:\n                    f.write(f\"{item}\\n\")\n        return f\"{file} exported\"\n</code></pre>"},{"location":"api_reference/validator_exporter/#datachecker.checks_loaders_and_exporters.validator_exporter.YAMLExporter","title":"<code>YAMLExporter</code>","text":"<p>               Bases: <code>Exporter</code></p> <p>YAMLExporter is a subclass of Exporter that handles exporting data to a YAML file. Methods</p> <p>init():     Initializes the YAMLExporter by setting the export format to 'yaml' and assigning the     export method. _export(data, file):     Static method that writes the provided data to the specified file in YAML format. Parameters</p> <p>data : Any     The data to be exported to YAML. file : str     The path to the file where the YAML data will be written. Returns</p> <p>str     A message indicating the file has been exported.</p> Source code in <code>datachecker/checks_loaders_and_exporters/validator_exporter.py</code> <pre><code>class YAMLExporter(Exporter):\n    \"\"\"\n    YAMLExporter is a subclass of Exporter that handles exporting data to a YAML file.\n    Methods\n    -------\n    __init__():\n        Initializes the YAMLExporter by setting the export format to 'yaml' and assigning the\n        export method.\n    _export(data, file):\n        Static method that writes the provided data to the specified file in YAML format.\n    Parameters\n    ----------\n    data : Any\n        The data to be exported to YAML.\n    file : str\n        The path to the file where the YAML data will be written.\n    Returns\n    -------\n    str\n        A message indicating the file has been exported.\n    \"\"\"\n\n    def __init__(self):\n        Exporter.__init__(self, \"yaml\", self._export)\n\n    @staticmethod\n    def _export(data, file):\n        with open(file, \"w\") as f:\n            yaml.dump(data, f, sort_keys=False)\n        return f\"{file} exported\"\n</code></pre>"},{"location":"contributor_guide/CODE_OF_CONDUCT/","title":"Code of conduct for <code>datachecker</code>","text":"<p>All contributors to this repository hosted by <code>ONS</code> are expected to follow the Contributor Covenant Code of Conduct. Those working within HM Government are also expected to follow the Civil Service Code.</p>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributor_guide/CODE_OF_CONDUCT/#definitions","title":"Definitions","text":"<p>Where this Code of Conduct says:</p> <ul> <li>\"Project\", we mean this repository <code>datachecker</code> ;</li> <li>\"Maintainer\", we mean active developers of the primary project team(s) behind <code>ONS</code>; and</li> <li>\"Leadership\", we mean <code>ONS</code> organisation owners, line managers, and other   leadership within HMG.</li> </ul>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people;</li> <li>Being respectful of differing opinions, viewpoints, and experiences;</li> <li>Giving and gracefully accepting constructive feedback;</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience;</li> <li>Focusing on what is best not just for us as individuals, but for the overall community.</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind;</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks;</li> <li>Public or private harassment;</li> <li>Publishing others\u2019 private information, such as a physical or email address, without their explicit permission;</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting.</li> </ul>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are expected to raise any instances of unacceptable behaviour to project leadership.</p> <p>Project leadership are responsible for clarifying the standards of acceptable behaviour. They also have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting using an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project leadership.</p>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team via the issues tab. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by members of the project's leadership.</p>"},{"location":"contributor_guide/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, and the <code>alphagov</code> Code of Conduct available at https://github.com/alphagov/.github/blob/main/CODE_OF_CONDUCT.md.</p>"},{"location":"contributor_guide/CONTRIBUTING/","title":"Contributing","text":"<p>We love contributions! We've compiled this documentation to help you understand our contributing guidelines. Please also read our <code>CODE_OF_CONDUCT.md</code>.</p>"},{"location":"contributor_guide/CONTRIBUTING/#getting-started","title":"Getting started","text":"<p>To start contributing, open your terminal and install the package and pre-commit hooks using:</p> <pre><code>pip install -e .[dev]\npre-commit install\n</code></pre> <p>The pre-commit hooks are a security feature to ensure, for example, no secrets, large data files, or Jupyter notebook outputs are accidentally committed into the repository. For more information and common use cases, please refer to a hook's documentation such as detect-secrets or nbstripout.</p>"},{"location":"contributor_guide/CONTRIBUTING/#code-conventions","title":"Code conventions","text":"<p>We mainly follow the GDS Way in our code conventions. For Python code, we follow the GDS Way Python style guide, and use the flake8 pre-commit hook for linting.</p>"},{"location":"contributor_guide/CONTRIBUTING/#git-and-github","title":"Git and GitHub","text":"<p>We use Git to version control the source code. Please read the Quality assurance of code for analysis and research for details on Git best practice. This includes how to write good commit messages, how to branch appropriately and solve merge conflicts.</p> <p>The .gitignore used in this repository was created with generic exclusions from gitignore.io.</p> <p>Pull requests into <code>main</code> require at least one approved review.</p>"},{"location":"contributor_guide/CONTRIBUTING/#markdown","title":"Markdown","text":"<p>Local links can be written as normal, but external links should be referenced at the bottom of the Markdown file for clarity. For example:</p> <p>Use a local link to reference the <code>using_pytest.md</code> file, but an external link for GOV.UK.</p> <p>We also try to wrap Markdown to a line length of 88 characters, but this is not strictly enforced in all cases, for example with long hyperlinks.</p>"},{"location":"contributor_guide/CONTRIBUTING/#testing","title":"Testing","text":"<p>Tests are written using the <code>pytest</code> framework, with its configuration in the <code>pyproject.toml</code> file. Note, only tests in the <code>tests</code> folder are run. To run the tests, enter the following command in your terminal:</p> <pre><code>pytest\n</code></pre>"},{"location":"contributor_guide/CONTRIBUTING/#code-coverage","title":"Code coverage","text":"<p>Code coverage of Python scripts is measured using the <code>coverage</code> Python package; its configuration can be found in <code>pyproject.toml</code>. Note coverage only extends to Python scripts in the <code>datachecker</code> folder.</p> <p>To run code coverage, and view it as an HTML report, enter the following command in your terminal:</p> <pre><code>coverage run -m pytest\ncoverage html\n</code></pre> <p>The HTML report can be accessed at <code>htmlcov/index.html</code>.</p>"},{"location":"contributor_guide/CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Documentation is stored in the <code>docs</code> folder unless it's more appropriate to store it elsewhere, like this contributing guidance. We write our documentation in MyST Markdown.</p>"},{"location":"contributor_guide/using_pytest/","title":"Using pytest","text":"<p>We use <code>pytest</code> to create and run all of our python based testing. Pytest is the most commonly used python module for testing python code. Testing your code is vital for followling coding best practices and has numerous benefits such as:</p> <ul> <li>They help to debug your code</li> <li>They make your write more efficient code first time</li> <li>They make you think about what precisely your code is doing as you right it</li> <li>They provide a sort of documentation for your code</li> <li>They help to keep your deployment smooth.</li> </ul>"},{"location":"contributor_guide/using_pytest/#structure","title":"Structure","text":"<p>There should be a <code>tests</code> folder in the root directory of your repository containing all the tests that relate to your package. It sits outside of your package because users that want to just use your package will not necessarily need the tests. The tests are there for contributers to use, and if they are contributing they will clone the whole repository, not just the package.</p> <p>There is an example pytest folder structure and file in your package that demonstrates this structure.</p>"},{"location":"contributor_guide/using_pytest/#writing-pytests","title":"Writing pytests","text":"<p>For pytest to find your tests, all test files and tests must either start with <code>test_</code> or finish with <code>_test.py</code></p> <p>The <code>test_example_module.py</code> example test file provides an example of these restrictions.</p>"},{"location":"contributor_guide/using_pytest/#running-pytest","title":"Running pytest","text":""},{"location":"contributor_guide/using_pytest/#in-the-terminal","title":"In the terminal","text":"<p>There are a few ways to run pytests in your terminal. The easiest is by running <pre><code>pytest\n</code></pre> in your root directory. This will find any existing pytests within your directory and run them.</p> <p>If you only want to run pytests in a specific pytest file you can run <pre><code>pytest tests/test_example_module.py\n</code></pre></p> <p>You can try both of these in the root directory of your new repository.</p>"},{"location":"contributor_guide/using_pytest/#further-reading","title":"Further reading","text":"<p>For more information on pytest, please visit (https://pypi.org/project/pytest/)</p>"},{"location":"user_guide/examples/","title":"Examples","text":""},{"location":"user_guide/examples/#validating-a-simple-dataset","title":"Validating a simple dataset","text":"<p>First we need to import the needed modules and load / create the schema. The contents of this schema could be saved in any form of configuration file (JSON, yaml, etc.) and be passed into our validator (example shown below)</p> <p>Our schema we are expecting age to be a <code>float</code> with minimum value of 0 and max of 120. Name is a <code>string</code> with at least 2 characters. Email is also a string but we are using a regular expression (Regex) to test the format is correct for a valid email. Finally we are expecting <code>is_active</code> to be an integer.</p> <pre><code>from datachecker import DataValidator\nimport pandas as pd\n\nschema = {\n    \"columns\": {\n        \"age\": {\n            \"type\": float,\n            \"min_val\":0,\n            \"max_val\":120,\n            \"allow_na\": False,\n            \"optional\": False\n        },\n        \"name\": {\n            \"type\": str,\n            \"min_len\": 2,\n            \"max_len\": 10,\n            \"allow_na\": False,\n            \"optional\": False,\n            \"allowed_strings\": r\"^[A-Za-z\\s]+$\"\n        },\n        \"email\": {\n            \"type\": str,\n            \"min_len\": 5,\n            \"max_len\": 50,\n            \"allow_na\": False,\n            \"optional\": False,\n            \"allowed_strings\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n        },\n        \"is_active\": {\n            \"type\": int,\n            \"allow_na\": False,\n            \"optional\": False\n        }\n    }\n}\n</code></pre> <p>Next we need to load our dataset, for this example we will instead create our dataframe within our script. Note for this example we have actually created the <code>is_active</code> column to be a boolean and not a integer as outlined in our schema. This should be picked up in our validation checks! Also one email is slightly incorrect and a name contains a number.</p> <pre><code>data = [\n    {\"age\": 30, \"name\": \"John Doe\", \"email\": \"john.doe@example.com\", \"is_active\": True},\n    {\"age\": 25, \"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\", \"is_active\": False},\n    {\"age\": 40, \"name\": \"Alice Brown\", \"email\": \"alice.brown.com\", \"is_active\": True},\n    {\"age\": -22, \"name\": \"Bob White\", \"email\": \"bob.white@example.com\", \"is_active\": False},\n    {\"age\": 35, \"name\": \"Carol Green1\", \"email\": \"carol.green@example.com\", \"is_active\": True},\n    {\"age\": 28, \"name\": \"Eve Black\", \"email\": \"eve.black@example.com\", \"is_active\": False}\n]\n\ndf = pd.DataFrame(data)\n</code></pre> <p>We can now run our validator and export our log. (IN THE FINAL VERSION VALIDATE AND EXPORT WILL BE DIRECTLY CALLED DURING CLASS INSTANTIATION) Printing the new_validtor object will print the contents of the log file to the terminal or python session.</p> <pre><code>new_validator = DataValidator(\n    schema = schema, \n    data=df,\n    file = \"output_report.yaml\",\n    format=\"yaml\")\n\nnew_validator.validate()\nnew_validator.export()\nprint(new_validator)\n</code></pre> <p>Now looking at the contents of the yaml or command line we can see our dataframe has passed most validation checks. </p> <pre><code>- date: '2025-12-23'\n  user: Omitted\n  device: Omitted\n  device_platform: Omitted\n  architecture: 64bit\n  python_version: 3.12.5\n  pandas_version: 2.3.3\n  pandera_version: 0.26.1\n  datachecker_version: 0.0.1\n- timestamp: '14:04:06'\n  description: Dataframe columns missing from schema\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: error\n- timestamp: '14:04:06'\n  description: Schema keys not in dataframe\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: warning\n- timestamp: '14:04:06'\n  description: checking column names\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: error\n- timestamp: '14:04:06'\n  description: checking column names are lowercase\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: warning\n- timestamp: '14:04:06'\n  description: checking mandatory columns are present\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: error\n- timestamp: '14:04:06'\n  description: checking for unexpected columns\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: warning\n- timestamp: '14:04:06'\n  description: Checking age dtype('float64')\n  outcome: Fail\n  failing_ids:\n  - int64\n  number_failing: 1\n  status: error\n- timestamp: '14:04:06'\n  description: Checking age greater_than_or_equal_to(0)\n  outcome: Fail\n  failing_ids:\n  - 3\n  number_failing: 1\n  status: error\n- timestamp: '14:04:06'\n  description: Checking age less_than_or_equal_to(120)\n  outcome: Pass\n  failing_ids: []\n  number_failing: 0\n  status: error\n- timestamp: '14:04:06'\n  description: Checking name str_matches('^[A-Za-z\\s]+$')\n  outcome: Fail\n  failing_ids:\n  - 4\n  number_failing: 1\n  status: error\n- timestamp: '14:04:06'\n  description: Checking email str_matches('^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n  outcome: Fail\n  failing_ids:\n  - 2\n  number_failing: 1\n  status: error\n- timestamp: '14:04:06'\n  description: Checking is_active dtype('int64')\n  outcome: Fail\n  failing_ids:\n  - bool\n  number_failing: 1\n  status: error\n</code></pre> <p>From out yaml output we can see it failed 5 checks. These were: </p> <pre><code>1. age was not a float, this was checked and found to be an integer\n2. not all ages were larger than 0, the entry in row 3 failed this check \n3. The name columns did not contain only upper or lowercase letters with spaces, entry in row 4 failed this.\n4. an invalid email address was found in row 2\n5. data type of is_active was a boolean when it was expecting an integer.\n</code></pre>"}]}